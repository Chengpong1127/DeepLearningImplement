{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyTorch import Model, Tensor\n",
    "from MyTorch.activations import ReLU, Softmax, GeLU\n",
    "from MyTorch.layers import Linear, Dropout, MultiheadAttention, LayerNorm, Embedding\n",
    "import numpy as np\n",
    "\n",
    "class GPT2_Layer(Model):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.attn = MultiheadAttention(d_model, num_heads, dropout)\n",
    "        self.drop1 = Dropout(dropout)\n",
    "        self.ln2 = LayerNorm(d_model)\n",
    "        self.ff1 = Linear(d_model, d_ff)\n",
    "        self.gelu1 = GeLU()\n",
    "        self.ff2 = Linear(d_ff, d_model)\n",
    "        self.drop2 = Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        o_x = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.attn(x, x, x, attention_mask)\n",
    "        x = self.drop1(x)\n",
    "        x = x + o_x\n",
    "        x = self.ln2(x)\n",
    "        o_x = x\n",
    "        x = self.ff1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.ff2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = x + o_x\n",
    "        return x\n",
    "\n",
    "class GPT2(Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout, vocab_size, max_len=1024):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = Embedding(max_len, d_model)\n",
    "        self.layers = [ GPT2_Layer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers) ]\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.final_linear = Linear(d_model, vocab_size)\n",
    "        self.softmax = Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "        x = self.ln1(x)\n",
    "        x = self.final_linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "class TextGenerator:\n",
    "    def __init__(self, gpt, tokenizer):\n",
    "        super().__init__()\n",
    "        self.gpt = gpt\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def generate_yield(self, input_str: str, max_len):\n",
    "        input_ids = self.tokenizer.encode(input_str)\n",
    "        input_ids = Tensor(input_ids)\n",
    "        input_ids = input_ids.reshape(1, -1)\n",
    "        attention_mask = Tensor(np.ones((1, input_ids.shape[1])))\n",
    "        for _ in range(max_len):\n",
    "            output = self.gpt(input_ids, attention_mask)\n",
    "            output = np.argmax(output.data, axis=-1)\n",
    "            output = output[:, -1:]\n",
    "            input_ids = Tensor(np.concatenate([input_ids.data, output], axis=1))\n",
    "            attention_mask = Tensor(np.ones((1, input_ids.shape[1])))\n",
    "            token = input_ids.data.astype(np.int32).reshape(-1)\n",
    "            new_str = self.tokenizer.decode(token)\n",
    "            yield new_str\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "    def generate(self, input_str: str, max_len) -> str:\n",
    "        return list(self.generate_yield(input_str, max_len))[-1]\n",
    "    \n",
    "    def generate_yield_word(self, input_str: str, max_len):\n",
    "        current_str = input_str\n",
    "        for new_str in self.generate_yield(input_str, max_len):\n",
    "            word = new_str[len(current_str):]\n",
    "            current_str = new_str\n",
    "            yield word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fulton held productivity consumeronian Lin HRopausal planners TM"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from MyTorch import Tensor\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "token = tokenizer([\"Hello, I'm a single sentence!\"])\n",
    "token = { k: Tensor(v) for k, v in token.items() }\n",
    "gpt2 = GPT2(2, 768, 12, 1024, 0.1, tokenizer.vocab_size)\n",
    "gpt2.eval()\n",
    "\n",
    "generator = TextGenerator(gpt2, tokenizer)\n",
    "\n",
    "for i in generator.generate_yield_word(\"Hello, I'm a single sentence!\", 10):\n",
    "    print(i, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
