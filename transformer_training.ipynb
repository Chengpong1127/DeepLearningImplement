{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from MyTorch import Model\n",
    "from MyTorch.operations import Flatten\n",
    "from MyTorch.activations import ReLU, Softmax\n",
    "from MyTorch.layers import Linear, Dropout, MultiheadAttention, LayerNorm, Attention\n",
    "\n",
    "\n",
    "class GPT2_Layer(Model):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiheadAttention(d_model, num_heads, dropout)\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.ln2 = LayerNorm(d_model)\n",
    "        self.drop1 = Dropout(dropout)\n",
    "        self.drop2 = Dropout(dropout)\n",
    "        self.ff = Linear(d_model, d_ff)\n",
    "        self.relu = ReLU()\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_out = self.attn(x, x, x, mask)\n",
    "        attn_out = self.drop1(attn_out)\n",
    "        x = self.ln1(x + attn_out)\n",
    "        ff_out = self.ff(x)\n",
    "        ff_out = self.relu(ff_out)\n",
    "        ff_out = self.drop2(ff_out)\n",
    "        x = self.ln2(x + ff_out)\n",
    "        return x\n",
    "\n",
    "class GPT2(Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = [GPT2_Layer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[[-0.17352307 -0.16763261 -0.17831038 ... -0.17326243 -0.17584415\n",
      "   -0.17637606]\n",
      "  [-0.18609305  5.4882216  -0.1806758  ... -0.22837962 -0.20461608\n",
      "   -0.19245334]\n",
      "  [-0.20495044 -0.16591473 -0.2060373  ... -0.20557004 -0.20573443\n",
      "   -0.20543458]\n",
      "  ...\n",
      "  [-0.1643044  -0.14986372 -0.15554751 ... -0.16656424 -0.27886516\n",
      "   -0.18949695]\n",
      "  [-0.20371205 -0.20282997 -0.20596766 ... -0.20230123 -0.20474124\n",
      "   -0.20490046]\n",
      "  [-0.06302877 -0.05055295 -0.05008615 ... -0.04764176 -0.05735055\n",
      "   -0.05499944]]], shape = (1, 10, 128))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from MyTorch import Tensor\n",
    "model = GPT2(2, 128, 8, 128, 0.1)\n",
    "\n",
    "input = Tensor(np.random.randn(1, 10, 128))\n",
    "output = model(input, None)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(2.2417287826538086, shape = ())\n"
     ]
    }
   ],
   "source": [
    "from MyTorch.losses import MSE\n",
    "loss = MSE()\n",
    "multihead = MultiheadAttention(8, 2)\n",
    "input = Tensor(np.random.randn(1, 10, 8))\n",
    "input2 = Tensor(np.random.randn(1, 10, 8))\n",
    "output = multihead(input, input2, input2, None)\n",
    "\n",
    "label = Tensor(np.random.randn(1, 10, 8))\n",
    "\n",
    "loss = loss(output, label)\n",
    "print(loss)\n",
    "loss.backward()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
