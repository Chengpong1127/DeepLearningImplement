{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyTorch import Model, Tensor\n",
    "from MyTorch.activations import Softmax, GeLU\n",
    "from MyTorch.layers import Linear, Dropout, MultiheadAttention, LayerNorm, Embedding\n",
    "import numpy as np\n",
    "\n",
    "class GPT2_Layer(Model):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.attn = MultiheadAttention(d_model, num_heads, dropout)\n",
    "        self.drop1 = Dropout(dropout)\n",
    "        self.ln2 = LayerNorm(d_model)\n",
    "        self.ff1 = Linear(d_model, d_ff)\n",
    "        self.gelu1 = GeLU()\n",
    "        self.ff2 = Linear(d_ff, d_model)\n",
    "        self.drop2 = Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        o_x = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.attn(x, x, x, attention_mask)\n",
    "        x = self.drop1(x)\n",
    "        x = x + o_x\n",
    "        x = self.ln2(x)\n",
    "        o_x = x\n",
    "        x = self.ff1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.ff2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = x + o_x\n",
    "        return x\n",
    "\n",
    "class GPT2(Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout, vocab_size, max_len=1024):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = Embedding(max_len, d_model)\n",
    "        self.layers = [ GPT2_Layer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers) ]\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.final_linear = Linear(d_model, vocab_size)\n",
    "        self.softmax = Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids)\n",
    "        pos_ids = Tensor(np.arange(x.shape[1]), requires_grad=False)\n",
    "        pos_embedding = self.pos_embedding(pos_ids)\n",
    "        x = x + pos_embedding\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "        x = self.ln1(x)\n",
    "        x = self.final_linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "class TextGenerator:\n",
    "    def __init__(self, gpt, tokenizer):\n",
    "        super().__init__()\n",
    "        self.gpt = gpt\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def generate_yield(self, input_str: str, max_len):\n",
    "        input_ids = self.tokenizer.encode(input_str)\n",
    "        input_ids = Tensor(input_ids)\n",
    "        input_ids = input_ids.reshape(1, -1)\n",
    "        attention_mask = Tensor(np.ones((1, input_ids.shape[1])))\n",
    "        for _ in range(max_len):\n",
    "            output = self.gpt(input_ids, attention_mask)\n",
    "            output = np.argmax(output.data, axis=-1)\n",
    "            output = output[:, -1:]\n",
    "            input_ids = Tensor(np.concatenate([input_ids.data, output], axis=1))\n",
    "            attention_mask = Tensor(np.ones((1, input_ids.shape[1])))\n",
    "            token = input_ids.data.astype(np.int32).reshape(-1)\n",
    "            new_str = self.tokenizer.decode(token)\n",
    "            yield new_str\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "    def generate(self, input_str: str, max_len) -> str:\n",
    "        return list(self.generate_yield(input_str, max_len))[-1]\n",
    "    \n",
    "    def generate_yield_word(self, input_str: str, max_len):\n",
    "        current_str = input_str\n",
    "        for new_str in self.generate_yield(input_str, max_len):\n",
    "            word = new_str[len(current_str):]\n",
    "            current_str = new_str\n",
    "            yield word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator(dataset):\n",
    "    for i, row in enumerate(dataset):\n",
    "        if i > 2:\n",
    "            break\n",
    "        for j in range(0, len(row[\"input_ids\"])):\n",
    "            yield {\"input_ids\": row[\"input_ids\"][:j+1], \"attention_mask\": row[\"attention_mask\"][:j+1]}\n",
    "\n",
    "def get_split_text_dataset(dataset):\n",
    "    from tqdm.auto import tqdm\n",
    "    from datasets import Dataset\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for row in tqdm(dataset):\n",
    "        for i in range(0, len(row[\"input_ids\"])):\n",
    "            input_ids.append(row[\"input_ids\"][:i+1])\n",
    "            attention_masks.append(row[\"attention_mask\"][:i+1])\n",
    "    \n",
    "    dataset = Dataset.from_dict({\"input_ids\": input_ids, \"attention_mask\": attention_masks})  \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "import datasets\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "dataset = datasets.load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"]) < 1024 and len(x[\"text\"]) > 20)\n",
    "dataset = dataset.filter(lambda x: x[\"text\"].encode(\"ascii\", \"ignore\").decode() == x[\"text\"])\n",
    "dataset = dataset.map(lambda x: tokenizer(x[\"text\"]))\n",
    "train_dataset = datasets.Dataset.from_generator(lambda: dataset_generator(dataset[\"train\"]))\n",
    "val_dataset = datasets.Dataset.from_generator(lambda: dataset_generator(dataset[\"validation\"]))\n",
    "\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", return_tensors='np')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", return_tensors='np')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 pretrain\n",
    "from transformers import AutoTokenizer\n",
    "from MyTorch import Tensor\n",
    "from MyTorch.optimizers import SGD\n",
    "from MyTorch.losses import CrossEntropy\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "gpt2 = GPT2(2, 768, 12, 1024, 0.1, tokenizer.vocab_size)\n",
    "optimizer = SGD(gpt2.get_parameters(), lr=0.001)\n",
    "loss_fn = CrossEntropy()\n",
    "\n",
    "epoches = 10\n",
    "batch_size = 32\n",
    "\n",
    "def train_step(gpt2: GPT2, tokenizer: AutoTokenizer, optimizer, loss_fn, input_ids, attention_masks):\n",
    "    intput_ids_tensor = Tensor(input_ids, requires_grad=False)\n",
    "    attention_masks_tensor = Tensor(attention_masks, requires_grad=False)\n",
    "    \n",
    "    output = gpt2(intput_ids_tensor, attention_masks_tensor)\n",
    "    \n",
    "    target = input_ids[:, 1:]\n",
    "    # to one hot\n",
    "    target = Tensor(np.eye(tokenizer.vocab_size)[target.astype(np.int32)], requires_grad= False)\n",
    "    output = output[:, :-1]\n",
    "    loss_value = loss_fn(output, target)\n",
    "    # cross entropy.shape = (batch, seq_len, vocab_size)\n",
    "    print(loss_value)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(1270.5062255859375, shape = ())\n",
      "Tensor(1613.028564453125, shape = ())\n",
      "Tensor(1736.511962890625, shape = ())\n",
      "Tensor(1803.906982421875, shape = ())\n",
      "Tensor(1555.4896240234375, shape = ())\n",
      "Tensor(1669.2515869140625, shape = ())\n",
      "Tensor(1638.6746826171875, shape = ())\n",
      "Tensor(2996.677001953125, shape = ())\n",
      "Tensor(1765.8671875, shape = ())\n",
      "Tensor(1723.6947021484375, shape = ())\n",
      "Tensor(1852.72119140625, shape = ())\n",
      "Tensor(1903.543701171875, shape = ())\n",
      "Tensor(1598.879150390625, shape = ())\n",
      "Tensor(1677.8656005859375, shape = ())\n",
      "Tensor(1631.4163818359375, shape = ())\n",
      "Tensor(2953.852294921875, shape = ())\n",
      "Tensor(1740.3922119140625, shape = ())\n",
      "Tensor(1696.293701171875, shape = ())\n",
      "Tensor(1879.939453125, shape = ())\n",
      "Tensor(1943.898681640625, shape = ())\n",
      "Tensor(1640.6026611328125, shape = ())\n",
      "Tensor(1739.9024658203125, shape = ())\n",
      "Tensor(1695.6492919921875, shape = ())\n",
      "Tensor(3065.319580078125, shape = ())\n",
      "Tensor(1813.2979736328125, shape = ())\n",
      "Tensor(1762.5687255859375, shape = ())\n",
      "Tensor(1889.9315185546875, shape = ())\n",
      "Tensor(1943.898681640625, shape = ())\n",
      "Tensor(1640.6026611328125, shape = ())\n",
      "Tensor(1739.9024658203125, shape = ())\n",
      "Tensor(1695.6492919921875, shape = ())\n",
      "Tensor(3065.319580078125, shape = ())\n",
      "Tensor(1813.2979736328125, shape = ())\n",
      "Tensor(1762.5687255859375, shape = ())\n",
      "Tensor(1889.9315185546875, shape = ())\n",
      "Tensor(1943.898681640625, shape = ())\n",
      "Tensor(1640.6026611328125, shape = ())\n",
      "Tensor(1739.9024658203125, shape = ())\n",
      "Tensor(1695.6492919921875, shape = ())\n",
      "Tensor(3065.319580078125, shape = ())\n",
      "Tensor(1813.2979736328125, shape = ())\n",
      "Tensor(1762.5687255859375, shape = ())\n",
      "Tensor(1889.9315185546875, shape = ())\n",
      "Tensor(1943.898681640625, shape = ())\n",
      "Tensor(1640.6026611328125, shape = ())\n",
      "Tensor(1739.9024658203125, shape = ())\n",
      "Tensor(1695.6492919921875, shape = ())\n",
      "Tensor(3065.319580078125, shape = ())\n",
      "Tensor(1813.2979736328125, shape = ())\n",
      "Tensor(1762.5687255859375, shape = ())\n",
      "Tensor(1889.9315185546875, shape = ())\n",
      "Tensor(1943.898681640625, shape = ())\n",
      "Tensor(1640.6026611328125, shape = ())\n",
      "Tensor(1739.9024658203125, shape = ())\n",
      "Tensor(1695.6492919921875, shape = ())\n",
      "Tensor(3065.319580078125, shape = ())\n",
      "Tensor(1813.2979736328125, shape = ())\n",
      "Tensor(1762.5687255859375, shape = ())\n",
      "Tensor(1889.9315185546875, shape = ())\n",
      "Tensor(1943.898681640625, shape = ())\n",
      "Tensor(1640.6026611328125, shape = ())\n",
      "Tensor(1739.9024658203125, shape = ())\n",
      "Tensor(1695.6492919921875, shape = ())\n",
      "Tensor(3065.319580078125, shape = ())\n",
      "Tensor(1813.2979736328125, shape = ())\n",
      "Tensor(1762.5687255859375, shape = ())\n",
      "Tensor(1889.9315185546875, shape = ())\n",
      "Tensor(1943.898681640625, shape = ())\n",
      "Tensor(1640.6026611328125, shape = ())\n",
      "Tensor(1739.9024658203125, shape = ())\n",
      "Tensor(1695.6492919921875, shape = ())\n",
      "Tensor(3065.319580078125, shape = ())\n",
      "Tensor(1813.2979736328125, shape = ())\n",
      "Tensor(1762.5687255859375, shape = ())\n",
      "Tensor(1889.9315185546875, shape = ())\n",
      "Tensor(1943.898681640625, shape = ())\n",
      "Tensor(1640.6026611328125, shape = ())\n",
      "Tensor(1739.9024658203125, shape = ())\n",
      "Tensor(1695.6492919921875, shape = ())\n",
      "Tensor(3065.319580078125, shape = ())\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "train_dataset = train_dataset.shuffle()\n",
    "for epoch in range(epoches):\n",
    "    for i in range(0, len(train_dataset), batch_size):\n",
    "        batch = train_dataset[i:i+batch_size]\n",
    "        batch = collate_fn(batch)\n",
    "        train_step(gpt2, tokenizer, optimizer, loss_fn, batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
